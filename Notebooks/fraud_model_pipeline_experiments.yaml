# PIPELINE DEFINITION
# Name: fraud-model-pipeline-experiments
# Description: Pipeline entrenamiento de modelos para el proyecto de detecciÃ³n de fraude bancario
# Inputs:
#    c_fn: float [Default: 20000.0]
#    c_fp: float [Default: 200.0]
#    c_review: float [Default: 30.0]
#    human_hit_rate: float [Default: 0.8]
#    model_display_name: str [Default: 'fraud-detection-model']
#    n_trials: int [Default: 50.0]
#    params_config_path: str
#    raw_data_path: str
#    test_size: float [Default: 0.1]
#    train_size: float [Default: 0.8]
#    val_size: float [Default: 0.1]
# Outputs:
#    calibrate-model-scenery_metrics: system.Metrics
#    calibrate-model-tune_model_metrics: system.ClassificationMetrics
#    evaluate-model-evaluate_metrics: system.ClassificationMetrics
#    evaluate-model-evaluate_metrics_path: system.Metrics
#    train-models-best_model_metrics: system.ClassificationMetrics
#    train-models-metrics_path: system.Metrics
#    tuning-model-tune_model_metrics: system.ClassificationMetrics
components:
  comp-calibrate-model:
    executorLabel: exec-calibrate-model
    inputDefinitions:
      artifacts:
        best_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        encoder_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        val_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        c_fn:
          defaultValue: 20000.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        c_fp:
          defaultValue: 200.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        c_review:
          defaultValue: 30.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        human_hit_rate:
          defaultValue: 0.8
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        scenery_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        tune_model_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        best_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        encode_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scenery_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        test_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        human_hit_rate:
          defaultValue: 0.8
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        evaluate_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        evaluate_metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        final_tuned_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-split-data:
    executorLabel: exec-split-data
    inputDefinitions:
      parameters:
        processed_data_path:
          parameterType: STRING
        test_size:
          defaultValue: 0.1
          isOptional: true
          parameterType: NUMBER_DOUBLE
        train_size:
          defaultValue: 0.8
          isOptional: true
          parameterType: NUMBER_DOUBLE
        val_size:
          defaultValue: 0.1
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        test_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-models:
    executorLabel: exec-train-models
    inputDefinitions:
      artifacts:
        train_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        best_model_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        encode_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-tuning-model:
    executorLabel: exec-tuning-model
    inputDefinitions:
      artifacts:
        encoder_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        train_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        n_trials:
          defaultValue: 50.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        params_config_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        tune_model_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        tuned_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-upload-model-to-vertex:
    executorLabel: exec-upload-model-to-vertex
    inputDefinitions:
      artifacts:
        best_model_metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        best_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        encode_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        experiment_name:
          defaultValue: fraud-detection-experiment
          isOptional: true
          parameterType: STRING
        model_display_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifacts:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-calibrate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - calibrate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef calibrate_model(\n    val_data_path: Input[Dataset],\n    best_model_path:\
          \ Input[Model],\n    encoder_path: Input[Model],\n    scenery_metrics: Output[Metrics],\n\
          \    tune_model_metrics: Output[ClassificationMetrics],\n    c_fn: float\
          \ = 20000,\n    c_fp: float = 200,\n    c_review: float = 30,\n    human_hit_rate:\
          \ float = 0.80,\n):\n    import pandas as pd\n    import numpy as np\n \
          \   from typing import Optional, Dict, Union\n    from sklearn.metrics import\
          \ roc_curve, roc_auc_score\n    import joblib\n    import json\n    import\
          \ os\n\n    def cost_function(\n            y_true: np.ndarray, \n     \
          \       y_proba: np.ndarray, \n            t_low_grid: np.ndarray, \n  \
          \          t_high_grid: np.ndarray,\n            c_fn: float = 10000, \n\
          \            c_fp: float = 200, \n            c_review: float = 30, \n \
          \           h: float = 0.80,\n            max_reviews: Optional[int] = None\n\
          \        ) -> Dict[str, Union[float, int]]:\n        \"\"\"\n        Calcula\
          \ el Umbral \xD3ptimo de Decisi\xF3n de Tres V\xEDas (3WD) que minimiza\
          \ el costo operacional total.\n\n        Esta funci\xF3n simula un sistema\
          \ de detecci\xF3n de fraude con tres resultados (Auto-Aprobar,\n       \
          \ Revisi\xF3n Humana, Auto-Rechazar) y encuentra la combinaci\xF3n de umbrales\
          \ (t_low, t_high)\n        que resulta en el menor costo de negocio, considerando\
          \ los costos asim\xE9tricos y la\n        capacidad de revisi\xF3n humana.\n\
          \n        Args:\n            y_true: Etiquetas reales de fraude (0 o 1).\n\
          \            y_proba: Probabilidades predichas por el modelo para la clase\
          \ positiva (fraude).\n            t_low_grid: Array de umbrales inferiores\
          \ a probar (p.ej., np.linspace(0.01, 0.4, 50)).\n            t_high_grid:\
          \ Array de umbrales superiores a probar (p.ej., np.linspace(0.4, 0.99, 50)).\n\
          \            c_fn: Costo de un Falso Negativo (Fraude no detectado). Alto\
          \ costo por defecto.\n            c_fp: Costo de un Falso Positivo (Transacci\xF3\
          n leg\xEDtima rechazada autom\xE1ticamente). Costo moderado.\n         \
          \   c_review: Costo de procesar un solo caso por el equipo de revisi\xF3\
          n humana. Bajo costo.\n            h: Eficacia (Hit Rate) del equipo de\
          \ revisi\xF3n humana (fracci\xF3n de fraudes que atrapan).\n           \
          \ max_reviews: L\xEDmite superior opcional para la cantidad de casos que\
          \ el equipo humano\n                        puede revisar. Si se excede,\
          \ la combinaci\xF3n de umbrales se ignora.\n\n        Returns:\n       \
          \     Un diccionario que contiene el costo m\xEDnimo (`cost`), los umbrales\
          \ \xF3ptimos (`t_low`, `t_high`)\n            y m\xE9tricas de desempe\xF1\
          o detalladas para ese umbral (TP, FP, FN, Recall, etc.).\n        \"\"\"\
          \n        best = {'cost': np.inf}\n        n = len(y_true)\n\n        real_fraud\
          \ = (y_true == 1)\n        real_no_fraud = (y_true == 0)\n        total_frauds\
          \ = real_fraud.sum()\n\n        for t_low in t_low_grid:\n            for\
          \ t_high in t_high_grid:\n\n                if t_low >= t_high:\n      \
          \              continue\n\n                # Segmentacion Umbrales\n   \
          \             auto_decline_mask = (y_proba >= t_high)\n                review_mask\
          \ = (y_proba < t_high) & (y_proba > t_low)\n                auto_approve_mask\
          \ = (y_proba <= t_low)\n\n                # Casos\n                TP_auto\
          \ = np.sum(auto_decline_mask & real_fraud)\n                FP_auto = np.sum(auto_decline_mask\
          \ & real_no_fraud)\n                FN_auto = np.sum(auto_approve_mask &\
          \ real_fraud)\n\n                frauds_in_review = np.sum(review_mask &\
          \ real_fraud)\n                no_frauds_in_review = np.sum(review_mask\
          \ & real_no_fraud)\n                review_count = np.sum(review_mask)\n\
          \n                # Capacidad\n                if (max_reviews is not None)\
          \ and (review_count > max_reviews):\n                    continue\n\n  \
          \              frauds_caught_by_review = h * frauds_in_review\n        \
          \        frauds_missed_by_review = (1 - h) * frauds_in_review\n        \
          \        FN_after = FN_auto + frauds_missed_by_review\n\n              \
          \  no_frauds_declined_by_review = (1 - h) * no_frauds_in_review\n      \
          \          FP_after = FP_auto + no_frauds_declined_by_review\n\n       \
          \         # Funcion de costo a minimizar \n\n                f_cost = c_fn\
          \ * FN_after + c_fp * FP_after + c_review * review_count\n\n           \
          \     # Resultados\n\n                if f_cost < best['cost']:\n      \
          \              best = {\n                        \"cost\": f_cost,\n   \
          \                     \"t_low\": t_low,\n                        \"t_high\"\
          : t_high,\n                        \"TP_auto\": int(TP_auto),\n        \
          \                \"FP_auto\": int(FP_auto),\n                        \"\
          frauds_in_review\": int(frauds_in_review),\n                        \"legit_in_review\"\
          : int(no_frauds_in_review),\n                        \"review_count\": int(review_count),\n\
          \                        \"FN_after\": float(FN_after),\n              \
          \          \"FP_after\": float(FP_after), # A\xF1adido para seguimiento\n\
          \                        \"recall_overall\": (TP_auto + frauds_caught_by_review)\
          \ / (total_frauds + 1e-9),\n                        \"precision_auto_decline\"\
          : TP_auto / (TP_auto + FP_auto + 1e-9),\n                        \"frac_review\"\
          : review_count / n\n                    }\n        return best\n\n    def\
          \ analyze_cost_function(best_results, total_samples, human_hit_rate):\n\n\
          \        TP_auto = best_results['TP_auto']\n        frauds_in_review = best_results['frauds_in_review']\n\
          \        FN_after = best_results['FN_after']\n        FP_after = best_results['FP_after']\n\
          \        FP_auto = best_results['FP_auto']\n        h = human_hit_rate\n\
          \n        FN_auto = FN_after - (1-h) * frauds_in_review\n\n\n        Total_Fraudes\
          \ = TP_auto + frauds_in_review + FN_auto\n        legit_in_review = best_results['legit_in_review']\n\
          \n        if total_samples is None:\n            total_samples = round(best_results['review_count']\
          \ / best_results['frac_review'])\n\n        Total_Fraudes = TP_auto + frauds_in_review\
          \ + FN_auto\n        Total_Legitimos = total_samples - Total_Fraudes\n \
          \       TN_auto = Total_Legitimos - legit_in_review - FP_auto\n\n      \
          \  confusion_matrix_data = [[TN_auto, legit_in_review, FP_auto],\n     \
          \                            [0,0,0],\n                                \
          \ [FN_auto, frauds_in_review, TP_auto]]\n\n        frauds_caught_by_review\
          \ = h * frauds_in_review\n        TP_after = TP_auto + frauds_caught_by_review\n\
          \n        recall_fraude = TP_after / (TP_after + FN_after + 1e-9)\n    \
          \    precision_fraude = TP_after / (TP_after + FP_after + 1e-9)\n      \
          \  f1_fraude = 2 * (precision_fraude * recall_fraude) / (precision_fraude\
          \ + recall_fraude + 1e-9)\n\n        return confusion_matrix_data, recall_fraude,\
          \ precision_fraude, f1_fraude\n\n    # Cargar los datos de validaci\xF3\
          n\n    data_val = pd.read_csv(f'{val_data_path.path}/val_data.csv')\n\n\
          \    # Cargar el encoder\n    encoder = joblib.load(f\"{encoder_path.path}/encoder.joblib\"\
          )\n\n    # Preparar los datos de validaci\xF3n\n    cat_features = data_val.select_dtypes(include=['object']).columns.tolist()\n\
          \    target = 'fraud_bool'\n\n    encoder_features_val = encoder.transform(data_val[cat_features])\n\
          \    encoded_df_val = pd.DataFrame(encoder_features_val, \n            \
          \                      columns=encoder.get_feature_names_out(cat_features),\n\
          \                                  index=data_val.index)\n\n    X_val =\
          \ pd.concat([data_val.drop(columns=cat_features + [target]), encoded_df_val],\
          \ axis=1)\n    y_val = data_val[target]\n\n    # Cargar el modelo\n    tuned_model\
          \ = joblib.load(f\"{best_model_path.path}/tuned_model.joblib\")\n\n    y_pred_proba\
          \ = tuned_model.predict_proba(X_val)[:, 1]\n\n    t_low_grid = np.linspace(0.01,\
          \ 0.2, 40)\n    t_high_grid = np.linspace(0.05, 0.5, 60)\n\n    results\
          \ = cost_function(y_val, y_pred_proba,\n                               \
          \ t_low_grid, t_high_grid,\n                                c_fn = c_fn,\n\
          \                                c_fp = c_fp,\n                        \
          \        c_review = c_review,\n                                h = human_hit_rate,\n\
          \                                max_reviews=2000)\n\n    # Treholds \xF3\
          ptimos\n    t_low_opt = results['t_low']\n    t_high_opt = results['t_high']\n\
          \n    # Metricas del modelo ajustado con la calibracion de la funcion de\
          \ costo\n    # log the confusion matrix\n    labels = ['No Fraude', 'Observado',\
          \ 'Fraude']\n\n    # y_pred_best = (y_pred_proba >= optimal_threshold).astype(int)\n\
          \    cm, recall, precision, f1 = analyze_cost_function(results, total_samples=None,\
          \ human_hit_rate=human_hit_rate)\n\n    tune_model_metrics.log_confusion_matrix(\n\
          \        categories=labels,\n        matrix=cm\n    )\n\n    # log roc auc\n\
          \    fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n\n    N_points\
          \ = 200\n    total_points = len(fpr)\n    indices = np.linspace(0, total_points\
          \ - 1, N_points, dtype = int)\n\n    fpr = np.nan_to_num(fpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    tpr = np.nan_to_num(tpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    thresholds = np.nan_to_num(thresholds[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n\n    tune_model_metrics.log_roc_curve(\n\
          \        fpr=fpr.tolist(),\n        tpr=tpr.tolist(),\n        threshold=thresholds.tolist()\n\
          \    )\n\n    calibrated_metrics = {\n            'recall': recall,\n  \
          \          'precision': precision,\n            'f1_score': f1,\n    }\n\
          \n    calibrated_metrics['roc_auc'] = float(roc_auc_score(y_val, y_pred_proba))\n\
          \    calibrated_metrics['t_low_opt'] = float(t_low_opt)\n    calibrated_metrics['t_high_opt']\
          \ = float(t_high_opt)\n    calibrated_metrics['cost_fn'] = float(c_fn)\n\
          \    calibrated_metrics['cost_fp'] = float(c_fp)\n    calibrated_metrics['cost_review']\
          \ = float(c_review)\n    calibrated_metrics['human_hit_rate'] = float(human_hit_rate)\n\
          \n    # log param metric\n    for metric_name, metric_value in calibrated_metrics.items():\n\
          \        scenery_metrics.log_metric(f'Calibrated_{metric_name}', metric_value)\n\
          \n    os.makedirs(scenery_metrics.path, exist_ok=True)\n    metrics_file_path\
          \ = scenery_metrics.path + \"/scenery_metrics.json\" \n    with open(metrics_file_path,\
          \ 'w') as f:\n        json.dump(calibrated_metrics, f, indent=4)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    test_data_path: Input[Dataset],\n    best_model_path:\
          \ Input[Model],\n    encode_path: Input[Model],\n    scenery_metrics: Input[Metrics],\n\
          \    final_tuned_model_path: Output[Model],\n    evaluate_metrics: Output[ClassificationMetrics],\n\
          \    evaluate_metrics_path: Output[Metrics],\n    human_hit_rate: float\
          \ = 0.80\n):\n\n    import pandas as pd\n    import numpy as np\n    from\
          \ sklearn.metrics import roc_auc_score, roc_curve\n    import joblib\n \
          \   import json\n    import os\n\n    # Cargar los datos de test\n    data_test\
          \ = pd.read_csv(f'{test_data_path.path}/test_data.csv')\n\n    # Cargar\
          \ el encoder y modelo\n    encoder = joblib.load(f\"{encode_path.path}/encoder.joblib\"\
          )\n    best_model = joblib.load(f\"{best_model_path.path}/tuned_model.joblib\"\
          )\n\n    # Preparar los datos de test\n    cat_features = data_test.select_dtypes(include=['object']).columns.tolist()\n\
          \    target = 'fraud_bool'\n\n    encoder_features_test = encoder.transform(data_test[cat_features])\n\
          \    encoded_df_test = pd.DataFrame(encoder_features_test, columns=encoder.get_feature_names_out(cat_features))\n\
          \n    X_test = pd.concat([data_test.drop(columns=cat_features + [target]),\
          \ encoded_df_test], axis=1)\n    y_test = data_test[target]\n\n    # Evaluar\
          \ el modelo\n    y_pred = best_model.predict(X_test)\n    y_pred_proba =\
          \ best_model.predict_proba(X_test)[:, 1]\n\n    # log the confusion matrix\n\
          \    labels = ['No Fraude', 'Observado', 'Fraude']\n\n    # Load optimal\
          \ treshold json\n    metrics_file = os.path.join(scenery_metrics.path, os.listdir(scenery_metrics.path)[0])\n\
          \    with open(metrics_file, \"r\") as f:\n        opt_tresholds = json.load(f)\n\
          \n    def results_model(y_test, y_pred_proba, opt_tresholds, human_hit_rate\
          \ = 0.8):\n\n        t_low_opt = opt_tresholds['t_low_opt']\n        t_high_opt\
          \ = opt_tresholds['t_high_opt']\n\n        n = len(y_test)\n\n        real_fraud\
          \ = (y_test == 1)\n        real_no_fraud = (y_test == 0)\n        total_frauds\
          \ = real_fraud.sum()\n\n        fraud_mask = (y_pred_proba >= t_high_opt)\n\
          \        review_mask = (y_pred_proba < t_high_opt) & (y_pred_proba > t_low_opt)\n\
          \        no_fraud_mask = (y_pred_proba <= t_low_opt)\n\n        TP_auto\
          \ = int(np.sum(fraud_mask & real_fraud))\n        FP_auto = int(np.sum(fraud_mask\
          \ & real_no_fraud))\n        FN_auto = int(np.sum(no_fraud_mask & real_fraud))\n\
          \        TN_auto = int(np.sum(no_fraud_mask & real_no_fraud))\n\n      \
          \  frauds_in_review = int(np.sum(review_mask & real_fraud))\n        legit_in_review\
          \ = int(np.sum(review_mask & real_no_fraud))\n        review_count = int(np.sum(review_mask))\n\
          \n        # Fraudes que el humano atrapa\n        frauds_caught_by_review\
          \ = human_hit_rate * frauds_in_review\n        # Fraudes que el humano pierde\
          \ (se convierten en FN final)\n        frauds_missed_by_review = (1 - human_hit_rate)\
          \ * frauds_in_review\n        # Leg\xEDtimos que el humano rechaza (se convierten\
          \ en FP final)\n        no_frauds_declined_by_review = (1 - human_hit_rate)\
          \ * legit_in_review\n\n        FN_total = FN_auto + frauds_missed_by_review\n\
          \        FP_total = FP_auto + no_frauds_declined_by_review\n        TP_total\
          \ = TP_auto + frauds_caught_by_review\n        Frauds_total = TP_total +\
          \ FN_total + frauds_in_review\n\n        recall_final = float(TP_total /\
          \ (Frauds_total + 1e-9))\n        precision_final = float(TP_total / (TP_total\
          \ + FP_total + 1e-9))\n        f1_final = float(2 * (precision_final * recall_final)\
          \ / (precision_final + recall_final + 1e-9))\n\n        confusion_matrix_data\
          \ = [\n            [TN_auto, legit_in_review, FP_auto], # Real No Fraude\n\
          \            [0,0,0],\n            [FN_auto, frauds_in_review, TP_auto]\
          \  # Real Fraude\n        ]\n\n        results = {\n            \"recall\"\
          : recall_final,\n            \"precision\": precision_final,\n         \
          \   \"f1_score\": f1_final,\n            \"review_fraction\": review_count\
          \ / n\n        }\n\n        return confusion_matrix_data, results\n\n\n\
          \    cm , results = results_model(\n        y_test, \n        y_pred_proba,\n\
          \        opt_tresholds,\n        human_hit_rate = human_hit_rate\n    )\n\
          \n    # log the confusion matrix\n    evaluate_metrics.log_confusion_matrix(\n\
          \        categories=labels,\n        matrix=cm\n    )\n\n    # Model\n \
          \   os.makedirs(final_tuned_model_path.path, exist_ok=True)\n    final_model_file_path\
          \ = final_tuned_model_path.path + \"/final_tuned_model.joblib\"\n    joblib.dump(best_model,\
          \ final_model_file_path)\n\n    # log roc auc\n    fpr, tpr, thresholds\
          \ = roc_curve(y_test, y_pred_proba)\n\n    N_points = 200\n    total_points\
          \ = len(fpr)\n    indices = np.linspace(0, total_points - 1, N_points, dtype\
          \ = int)\n\n    fpr = np.nan_to_num(fpr[indices], nan=0.0, posinf=1.0, neginf=0.0)\n\
          \    tpr = np.nan_to_num(tpr[indices], nan=0.0, posinf=1.0, neginf=0.0)\n\
          \    thresholds = np.nan_to_num(thresholds[indices], nan=0.0, posinf=1.0,\
          \ neginf=0.0)\n\n    evaluate_metrics.log_roc_curve(\n        fpr=fpr.tolist(),\n\
          \        tpr=tpr.tolist(),\n        threshold=thresholds.tolist()\n    )\
          \ \n\n    # log metric\n    results['roc_auc'] = float(roc_auc_score(y_test,\
          \ y_pred_proba))\n    results['t_low_opt'] = opt_tresholds['t_low_opt']\n\
          \    results['t_high_opt'] = opt_tresholds['t_high_opt']\n    results['human_hit_rate']\
          \ = human_hit_rate\n    results['cost_fn'] = opt_tresholds['cost_fn']\n\
          \    results['cost_fp'] = opt_tresholds['cost_fp']\n    results['cost_review']\
          \ = opt_tresholds['cost_review']\n\n    for metric_name, metric_value in\
          \ results.items():\n        evaluate_metrics_path.log_metric(f'Calibrated_{metric_name}',\
          \ metric_value)\n\n    os.makedirs(evaluate_metrics_path.path, exist_ok=True)\n\
          \    metrics_file_path = evaluate_metrics_path.path + \"/model_metrics.json\"\
          \n    with open(metrics_file_path, 'w') as f:\n        json.dump(results,\
          \ f, indent=4)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-split-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - split_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef split_data(\n    processed_data_path: str,\n    train_data_path:\
          \ Output[Dataset],\n    test_data_path: Output[Dataset],\n    val_data_path:\
          \ Output[Dataset],\n    train_size: float = 0.8,\n    val_size: float =\
          \ 0.1,\n    test_size: float = 0.1\n):\n    import pandas as pd\n    from\
          \ sklearn.model_selection import train_test_split\n    import os\n\n   \
          \ # Cargar los datos procesados\n    data = pd.read_csv(processed_data_path)\n\
          \n    # Diividir los datos en train, validation y test\n    train, val_test\
          \ = train_test_split(\n                                data, test_size =\
          \ (1 - train_size), \n                                random_state=42, \n\
          \                                stratify=data['fraud_bool']\n         \
          \                       )\n    val, test = train_test_split(\n         \
          \                       val_test, test_size = (test_size / (val_size + test_size)),\
          \ \n                                random_state=42, \n                \
          \                stratify=val_test['fraud_bool']\n                     \
          \           )\n\n    # Guardar los conjuntos de datos\n    os.makedirs(train_data_path.path,\
          \ exist_ok=True)\n    os.makedirs(val_data_path.path, exist_ok=True)\n \
          \   os.makedirs(test_data_path.path, exist_ok=True)\n\n    train_data_path\
          \ = train_data_path.path + \"/train_data.csv\"\n    val_data_path = val_data_path.path\
          \ + \"/val_data.csv\"\n    test_data_path = test_data_path.path + \"/test_data.csv\"\
          \n\n    train.to_csv(train_data_path, index=False)\n    val.to_csv(val_data_path,\
          \ index=False)\n    test.to_csv(test_data_path, index=False)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-train-models:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_models
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_models(\n    train_data_path: Input[Dataset],\n    val_data_path:\
          \ Input[Dataset],\n    encode_path: Output[Model],\n    best_model_metrics:\
          \ Output[ClassificationMetrics],\n    metrics_path: Output[Metrics],\n):\n\
          \    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing\
          \ import OneHotEncoder\n    from sklearn.ensemble import RandomForestClassifier\n\
          \    from sklearn.metrics import accuracy_score, precision_score, recall_score,\
          \ f1_score, roc_auc_score, confusion_matrix, roc_curve\n    from xgboost\
          \ import XGBClassifier\n    from lightgbm import LGBMClassifier\n    import\
          \ joblib\n    import json\n    import os\n\n    # Cargar los datos de entrenamiento\n\
          \    data = pd.read_csv(f'{train_data_path.path}/train_data.csv')\n\n  \
          \  # Crear encoder, separar caracter\xEDsticas y etiqueta\n    cat_features\
          \ = data.select_dtypes(include=['object']).columns.tolist()\n    target\
          \ = 'fraud_bool'\n\n    encoder = OneHotEncoder(handle_unknown='ignore',\
          \ sparse_output=False)\n    encoder_features = encoder.fit_transform(data[cat_features])\n\
          \    encoded_df = pd.DataFrame(encoder_features, \n                    \
          \          columns=encoder.get_feature_names_out(cat_features),\n      \
          \                        index=data.index)\n\n    X_train = pd.concat([data.drop(columns=cat_features\
          \ + [target]), encoded_df], axis=1)\n    y_train = data[target]\n\n    #\
          \ Balancear clases\n    neg, pos = y_train.value_counts()[0], y_train.value_counts()[1]\n\
          \    scale_pos_weight = neg / pos\n\n    # Definir y entrenar modelos\n\
          \    models = {\n        'RandomForestClassifier': RandomForestClassifier(\n\
          \            n_estimators=100,\n            class_weight='balanced', \n\
          \            random_state=42\n            ),\n        'XGBClassifier': XGBClassifier(\n\
          \            eval_metric='logloss', \n            scale_pos_weight=scale_pos_weight,\
          \ \n            random_state=42\n            ),\n        'LGBMClassifier':\
          \ LGBMClassifier(\n            scale_pos_weight=scale_pos_weight,\n    \
          \        random_state=42\n            )\n    }\n\n    # Guardar el encoder\n\
          \    os.makedirs(encode_path.path, exist_ok=True)\n    encode_path = encode_path.path\
          \ + \"/encoder.joblib\"\n    joblib.dump(encoder, encode_path)\n\n\n   \
          \ # Evaluate models\n    data_val = pd.read_csv(f'{val_data_path.path}/val_data.csv')\n\
          \    encoder_features_val = encoder.transform(data_val[cat_features])\n\
          \    encoded_df = pd.DataFrame(encoder_features_val, \n                \
          \              columns=encoder.get_feature_names_out(cat_features),\n  \
          \                            index=data_val.index)\n\n    X_val = pd.concat([data_val.drop(columns=cat_features\
          \ + [target]), encoded_df], axis=1)\n    y_val = data_val[target]\n\n  \
          \  all_metrics = {}\n    best_model = None\n    best_f1 = -1\n\n    for\
          \ name, model in models.items():\n        model.fit(X_train, y_train)\n\
          \        y_pred = model.predict(X_val)\n        y_proba = model.predict_proba(X_val)[:,\
          \ 1]\n\n        f1 = f1_score(y_val, y_pred)\n\n        all_metrics[name]\
          \ = {\n            'accuracy': accuracy_score(y_val, y_pred),\n        \
          \    'precision': precision_score(y_val, y_pred),\n            'recall':\
          \ recall_score(y_val, y_pred),\n            'f1_score': f1,\n          \
          \  'roc_auc': roc_auc_score(y_val, y_proba)\n        }\n\n        print(f\"\
          {name} - F1 Score: {f1}\")\n        print(f\"{name} - Accuracy: {all_metrics[name]}\"\
          )\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_model\
          \ = model\n\n    # log the confusion matrix\n    labels = ['No Fraude',\
          \ 'Fraude']\n\n    y_pred_best = best_model.predict(X_val)\n    cm = confusion_matrix(y_val,\
          \ y_pred_best)\n    confusion_matrix_data = cm.tolist()\n\n    best_model_metrics.log_confusion_matrix(\n\
          \        categories=labels,\n        matrix=confusion_matrix_data\n    )\n\
          \n    # log roc auc\n    y_pred_proba = best_model.predict_proba(X_val)[:,\
          \ 1]\n    fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n\n    N_points\
          \ = 200\n    total_points = len(fpr)\n    indices = np.linspace(0, total_points\
          \ - 1, N_points, dtype = int)\n\n    fpr = np.nan_to_num(fpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    tpr = np.nan_to_num(tpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    thresholds = np.nan_to_num(thresholds[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n\n    best_model_metrics.log_roc_curve(\n\
          \        fpr=fpr.tolist(),\n        tpr=tpr.tolist(),\n        threshold=thresholds.tolist()\n\
          \    )\n\n    # log param metric\n    for name, metrics_dict in all_metrics.items():\n\
          \        metrics_path.log_metric(f'{name}_f1_score', metrics_dict.get('f1_score'))\n\
          \        metrics_path.log_metric(f'{name}_roc_auc', metrics_dict.get('roc_auc'))\n\
          \n    os.makedirs(metrics_path.path, exist_ok=True)\n    metrics_file_path\
          \ = metrics_path.path + \"/models_metrics.json\" \n    with open(metrics_file_path,\
          \ 'w') as f:\n        json.dump(all_metrics, f, indent=4)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-tuning-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - tuning_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef tuning_model(\n    train_data_path: Input[Dataset],\n    val_data_path:\
          \ Input[Dataset],\n    encoder_path: Input[Model],\n    metrics_path: Input[Metrics],\n\
          \    params_config_path: str,\n    tuned_model_path: Output[Model],\n  \
          \  tune_model_metrics: Output[ClassificationMetrics],\n    n_trials: int\
          \ = 50,\n):\n    import os\n    import pandas as pd\n    import numpy as\
          \ np\n    import joblib\n    import optuna\n    import yaml\n    import\
          \ json\n    from sklearn.metrics import accuracy_score, precision_score,\
          \ recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve\n\
          \    from xgboost import XGBClassifier\n    from lightgbm import LGBMClassifier\
          \  \n    from sklearn.ensemble import RandomForestClassifier\n    import\
          \ gcsfs\n    import lightgbm as lgb\n    from sklearn.model_selection import\
          \ StratifiedKFold\n\n    # Cargar los datos de validaci\xF3n\n    data_train\
          \ = pd.read_csv(f'{train_data_path.path}/train_data.csv')\n    data_val\
          \ = pd.read_csv(f'{val_data_path.path}/val_data.csv')\n\n    # Cargar el\
          \ encoder\n    encoder = joblib.load(f\"{encoder_path.path}/encoder.joblib\"\
          )\n\n    # Preparar los datos de validaci\xF3n\n    cat_features = data_train.select_dtypes(include=['object']).columns.tolist()\n\
          \    target = 'fraud_bool'\n\n    encoder_features_train = encoder.transform(data_train[cat_features])\n\
          \    encoded_df_train = pd.DataFrame(encoder_features_train, \n        \
          \                            columns=encoder.get_feature_names_out(cat_features),\n\
          \                                    index=data_train.index)\n\n    X_train\
          \ = pd.concat([data_train.drop(columns=cat_features + [target]), encoded_df_train],\
          \ axis=1)\n    y_train = data_train[target]\n\n    encoder_features_val\
          \ = encoder.transform(data_val[cat_features])\n    encoded_df_val = pd.DataFrame(encoder_features_val,\
          \ \n                                  columns=encoder.get_feature_names_out(cat_features),\n\
          \                                  index=data_val.index)\n\n    X_val =\
          \ pd.concat([data_val.drop(columns=cat_features + [target]), encoded_df_val],\
          \ axis=1)\n    y_val = data_val[target]\n\n    # Balancear clases\n    neg,\
          \ pos = y_train.value_counts()[0], y_train.value_counts()[1]\n    scale_pos_weight\
          \ = neg / pos\n\n    # Cargar el yaml de hiperpar\xE1metros\n    def download_yaml_from_gcs(gcs_path:\
          \ str, local_path: str = \"config.yaml\"):\n        fs = gcsfs.GCSFileSystem()\n\
          \        fs.get(gcs_path, local_path)\n\n        with open(local_path, \"\
          r\") as f:\n            config = yaml.safe_load(f)\n        return config\n\
          \n    params_config = download_yaml_from_gcs(params_config_path)\n\n   \
          \ # Load metrics json\n    metrics_file = os.path.join(metrics_path.path,\
          \ os.listdir(metrics_path.path)[0])\n    with open(metrics_file, \"r\")\
          \ as f:\n        metrics = json.load(f)\n\n    best_model = max(metrics.items(),\
          \ key=lambda x: x[1][\"f1_score\"])\n    best_model_name = best_model[0]\n\
          \n    # Definir hiperpar\xE1metros para ajustar\n    def objective(trial,\
          \ X: pd.DataFrame, y: pd.Series):\n\n        param_type = {\n          \
          \  'n_estimators': 'int', 'max_depth': 'int', 'min_samples_split': 'int',\n\
          \            'min_samples_leaf': 'int', 'num_leaves': 'int',\n         \
          \   'learning_rate': 'float', 'subsample': 'float', 'colsample_bytree':\
          \ 'float',\n            'reg_alpha': 'float', 'reg_lambda': 'float', 'gamma':\
          \ 'float',\n            'feature_fraction': 'float', 'bagging_fraction':\
          \ 'float', 'lambda_l1': 'float',\n            'lambda_l2': 'float',\n  \
          \          'max_features': 'categorical', 'bootstrap': 'categorical', 'class_weight':\
          \ 'categorical',\n            'objective': 'constant'\n        }\n\n   \
          \     params = {}\n\n        model_config = params_config.get(best_model_name)\n\
          \n        for param_name, value in model_config.items():\n\n           \
          \ if param_type.get(param_name) == 'int':\n                params[param_name]\
          \ = trial.suggest_int(param_name, value[0], value[1])\n            elif\
          \ param_type.get(param_name) == 'float':\n                params[param_name]\
          \ = trial.suggest_float(param_name, value[0], value[1])\n            elif\
          \ param_type.get(param_name) == 'categorical':\n                params[param_name]\
          \ = trial.suggest_categorical(param_name, value)\n            elif param_type.get(param_name)\
          \ == 'constant':\n                params[param_name] = value\n         \
          \   else:\n                raise ValueError(f\"Tipo de par\xE1metro no soportado:\
          \ {param_type[param_name]}\")\n\n        if best_model_name in ['LGBMClassifier',\
          \ 'XGBClassifier']:\n            params['scale_pos_weight'] = scale_pos_weight\n\
          \n        # validacion cruzada estratificada\n\n        skf = StratifiedKFold(n_splits=5,\
          \ shuffle=True, random_state=42)\n        f1_scores = []\n\n        for\
          \ train_index, val_index in skf.split(X, y):\n            X_train_fold,\
          \ X_val_fold = X.iloc[train_index], X.iloc[val_index]\n            y_train_fold,\
          \ y_val_fold = y.iloc[train_index], y.iloc[val_index]\n\n            # Crear\
          \ la instancia del modelo din\xE1micamente\n            if best_model_name\
          \ == 'RandomForestClassifier':\n                model = RandomForestClassifier(**params,\
          \ random_state=42)\n            elif best_model_name == 'LGBMClassifier':\n\
          \                model = LGBMClassifier(**params, random_state=42, verbose=-1)\n\
          \            elif best_model_name == 'XGBClassifier':\n                model\
          \ = XGBClassifier(**params, random_state=42, verbose=0, use_label_encoder=False)\n\
          \            else:\n                raise ValueError(f\"Modelo no soportado\
          \ para tuning: {best_model_name}\")\n\n            if best_model_name ==\
          \ 'RandomForestClassifier':\n                model.fit(X_train_fold, y_train_fold)\n\
          \            elif best_model_name == 'LGBMClassifier':\n               \
          \ model.fit(X_train_fold, y_train_fold,\n                            eval_set=[(X_val_fold,\
          \ y_val_fold)],\n                            )\n            elif best_model_name\
          \ == 'XGBClassifier':\n                model.fit(X_train_fold, y_train_fold,\n\
          \                            eval_set=[(X_val_fold, y_val_fold)],\n    \
          \                        verbose=False,\n                            )\n\
          \n            # Evaluar el modelo  \n            y_pred_proba_trial = model.predict_proba(X_val)[:,\
          \ 1]\n            precision_trial, recall_trial, _ = precision_recall_curve(y_val,\
          \ y_pred_proba_trial)\n\n            f1_scores_fold = 2 * (precision_trial\
          \ * recall_trial) / (precision_trial + recall_trial + 1e-9)\n\n        \
          \    best_f1 = np.max(f1_scores_fold)\n            f1_scores.append(best_f1)\n\
          \n        return np.mean(f1_scores)\n\n    # Ejecutar la optimizaci\xF3\
          n de hiperpar\xE1metros\n    study = optuna.create_study(direction='maximize')\n\
          \    func = lambda trial: objective(trial, X_train, y_train)\n    study.optimize(func,\
          \ n_trials=n_trials)\n\n    # Entrenar el modelo con los mejores hiperpar\xE1\
          metros\n    best_params = study.best_trial.params\n\n\n    if best_model_name\
          \ == 'RandomForestClassifier':\n        tuned_model = RandomForestClassifier(**best_params,\
          \ random_state=42)\n        tuned_model.fit(X_train, y_train)\n    elif\
          \ best_model_name == 'LGBMClassifier':\n        tuned_model = LGBMClassifier(**best_params,\
          \ random_state=42, verboose=-1)\n        tuned_model.fit(\n            X_train,\
          \ y_train,\n            eval_set=[(X_val, y_val)],\n            verbose=False,\n\
          \            early_stopping_rounds=10\n        )\n    elif best_model_name\
          \ == 'XGBClassifier':\n        tuned_model = XGBClassifier(**best_params,\
          \ random_state=42, verbose=0, use_label_encoder=False)\n        tuned_model.fit(\n\
          \            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n\
          \            verbose=False\n        )\n    else:\n        raise ValueError(f\"\
          Modelo no soportado: {best_model_name}\")\n\n    # Validacion del modelo\
          \ ajustado\n\n    y_pred = tuned_model.predict(X_val)\n    y_pred_proba\
          \ = tuned_model.predict_proba(X_val)[:, 1]    \n    precision_final, recall_final,\
          \ thresholds_final = precision_recall_curve(y_val, y_pred_proba)\n\n   \
          \ f1_scores = 2 * (precision_final * recall_final) / (precision_final +\
          \ recall_final + 1e-9)\n\n    best_threshold_index = np.argmax(f1_scores)\n\
          \n    if best_threshold_index == len(thresholds_final):\n         optimal_threshold\
          \ = thresholds_final[-1] # \xDAltimo umbral\n    else:\n         optimal_threshold\
          \ = thresholds_final[best_threshold_index]\n\n    # Metricas del modelo\
          \ ajustado\n    # log the confusion matrix\n    labels = ['No Fraude', 'Fraude']\n\
          \n    y_pred_best = (y_pred_proba >= optimal_threshold).astype(int)\n  \
          \  cm = confusion_matrix(y_val, y_pred_best)\n    confusion_matrix_data\
          \ = cm.tolist()\n\n    tune_model_metrics.log_confusion_matrix(\n      \
          \  categories=labels,\n        matrix=confusion_matrix_data\n    )\n\n \
          \   # log roc auc\n    fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n\
          \n    N_points = 200\n    total_points = len(fpr)\n    indices = np.linspace(0,\
          \ total_points - 1, N_points, dtype = int)\n\n    fpr = np.nan_to_num(fpr[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n    tpr = np.nan_to_num(tpr[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n    thresholds = np.nan_to_num(thresholds[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n\n    tune_model_metrics.log_roc_curve(\n\
          \        fpr=fpr.tolist(),\n        tpr=tpr.tolist(),\n        threshold=thresholds.tolist()\n\
          \    )\n\n    # Metadata del modelo ajustado\n    tuned_model_path.metadata['model']\
          \ = best_model_name\n    tuned_model_path.metadata['ROC-AUC'] = roc_auc_score(y_val,\
          \ y_pred_proba)\n    tuned_model_path.metadata['Optimal_Threshold'] = float(optimal_threshold)\n\
          \    tuned_model_path.metadata['F1-score'] = f1_score(y_val, y_pred_best)\n\
          \    tuned_model_path.metadata['Recall'] = recall_score(y_val, y_pred_best)\n\
          \    tuned_model_path.metadata['Precision'] = precision_score(y_val, y_pred_best)\n\
          \n    # Guardar el modelo ajustado\n    os.makedirs(tuned_model_path.path,\
          \ exist_ok=True)\n    tuned_model_file = tuned_model_path.path + \"/tuned_model.joblib\"\
          \n    joblib.dump(tuned_model, tuned_model_file)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-upload-model-to-vertex:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_vertex
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_vertex(\n    best_model_path: Input[Model],\n\
          \    best_model_metrics_path: Input[Metrics],\n    encode_path: Input[Model],\n\
          \    artifacts: Output[Artifact],\n    model_display_name: str,\n    experiment_name:\
          \ str = 'fraud-detection-experiment'\n) :\n\n    import google.cloud.aiplatform\
          \ as aiplatform\n    import datetime\n    import shutil\n    import json\n\
          \    import joblib\n    import os\n    from pathlib import Path\n\n\n  \
          \  # Artifact\n    os.makedirs(artifacts.path, exist_ok = True)\n\n    #\
          \ Cargar encoder al artifact path\n    encoder_src = Path(encode_path.path)\
          \ / 'encoder.joblib'\n    encoder_artifact = Path(artifacts.path) / 'encoder.joblib'\n\
          \    shutil.copy(encoder_src, encoder_artifact)\n\n    # Cargar Modelo\n\
          \    model_source = Path(best_model_path.path)\n    model_files = [f for\
          \ f in  model_source.iterdir() if f.is_file()]\n\n    if model_files:\n\
          \        model_file_path = model_files[0]\n        model_artifact = Path(artifacts.path)\
          \ / 'final_model.joblib'\n        shutil.copy(model_file_path, model_artifact)\n\
          \        print(f'Modelo copiado: {model_file_path.name} -> final_tuned_model.joblib')\n\
          \    else:\n        raise FileNotFoundError(\"Error: No se encontr\xF3 el\
          \ archivo del modelo.\")\n\n    # Cargar metricas\n    metrics_source_dir\
          \ = Path(best_model_metrics_path.path)\n    metrics_files = [f for f in\
          \ metrics_source_dir.iterdir() if f.is_file()] \n\n    if metrics_files:\n\
          \        metrics_file = metrics_files[0]\n        # Cargamos las m\xE9tricas\
          \ para loguearlas en el experimento\n        with open(metrics_file, \"\
          r\") as f:\n            metrics = json.load(f)\n        shutil.copy(metrics_file,\
          \ Path(artifacts.path) / \"model_metrics.json\")\n    else:\n        print(\"\
          Advertencia: No se encontr\xF3 el archivo de m\xE9tricas. Logueando m\xE9\
          tricas vac\xEDas.\")\n        metrics = {}\n\n\n    # Inicializar Vertex\
          \ AI\n    aiplatform.init()\n\n    # Crear Experimento\n    aiplatform.init(experiment=experiment_name)\n\
          \    run_name = f\"run-{model_display_name}-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\
          \n    run = aiplatform.start_run(run = run_name)\n\n    # Subir el modelo\
          \ a Vertex AI\n    artifact = aiplatform.Model.upload(\n        display_name=model_display_name,\n\
          \        artifact_uri=artifacts.path,\n        # artifact_uri=best_model_path.path.rsplit('/',\
          \ 1)[0],\n        serving_container_image_uri='us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest'\n\
          \    )\n\n    aiplatform.log_metrics(metrics)\n\n    aiplatform.end_run()\n\
          \n    print(f'Modelo subido a Vertex AI con ID: {artifact.resource_name}')\n\
          \n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
pipelineInfo:
  description: "Pipeline entrenamiento de modelos para el proyecto de detecci\xF3\
    n de fraude bancario"
  name: fraud-model-pipeline-experiments
root:
  dag:
    outputs:
      artifacts:
        calibrate-model-scenery_metrics:
          artifactSelectors:
          - outputArtifactKey: scenery_metrics
            producerSubtask: calibrate-model
        calibrate-model-tune_model_metrics:
          artifactSelectors:
          - outputArtifactKey: tune_model_metrics
            producerSubtask: calibrate-model
        evaluate-model-evaluate_metrics:
          artifactSelectors:
          - outputArtifactKey: evaluate_metrics
            producerSubtask: evaluate-model
        evaluate-model-evaluate_metrics_path:
          artifactSelectors:
          - outputArtifactKey: evaluate_metrics_path
            producerSubtask: evaluate-model
        train-models-best_model_metrics:
          artifactSelectors:
          - outputArtifactKey: best_model_metrics
            producerSubtask: train-models
        train-models-metrics_path:
          artifactSelectors:
          - outputArtifactKey: metrics_path
            producerSubtask: train-models
        tuning-model-tune_model_metrics:
          artifactSelectors:
          - outputArtifactKey: tune_model_metrics
            producerSubtask: tuning-model
    tasks:
      calibrate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-calibrate-model
        dependentTasks:
        - split-data
        - train-models
        - tuning-model
        inputs:
          artifacts:
            best_model_path:
              taskOutputArtifact:
                outputArtifactKey: tuned_model_path
                producerTask: tuning-model
            encoder_path:
              taskOutputArtifact:
                outputArtifactKey: encode_path
                producerTask: train-models
            val_data_path:
              taskOutputArtifact:
                outputArtifactKey: val_data_path
                producerTask: split-data
          parameters:
            c_fn:
              componentInputParameter: c_fn
            c_fp:
              componentInputParameter: c_fp
            c_review:
              componentInputParameter: c_review
            human_hit_rate:
              componentInputParameter: human_hit_rate
        taskInfo:
          name: calibrate-model
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - calibrate-model
        - split-data
        - train-models
        - tuning-model
        inputs:
          artifacts:
            best_model_path:
              taskOutputArtifact:
                outputArtifactKey: tuned_model_path
                producerTask: tuning-model
            encode_path:
              taskOutputArtifact:
                outputArtifactKey: encode_path
                producerTask: train-models
            scenery_metrics:
              taskOutputArtifact:
                outputArtifactKey: scenery_metrics
                producerTask: calibrate-model
            test_data_path:
              taskOutputArtifact:
                outputArtifactKey: test_data_path
                producerTask: split-data
          parameters:
            human_hit_rate:
              componentInputParameter: human_hit_rate
        taskInfo:
          name: evaluate-model
      split-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-split-data
        inputs:
          parameters:
            processed_data_path:
              componentInputParameter: raw_data_path
            test_size:
              componentInputParameter: test_size
            train_size:
              componentInputParameter: train_size
            val_size:
              componentInputParameter: val_size
        taskInfo:
          name: split-data
      train-models:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-models
        dependentTasks:
        - split-data
        inputs:
          artifacts:
            train_data_path:
              taskOutputArtifact:
                outputArtifactKey: train_data_path
                producerTask: split-data
            val_data_path:
              taskOutputArtifact:
                outputArtifactKey: val_data_path
                producerTask: split-data
        taskInfo:
          name: train-models
      tuning-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-tuning-model
        dependentTasks:
        - split-data
        - train-models
        inputs:
          artifacts:
            encoder_path:
              taskOutputArtifact:
                outputArtifactKey: encode_path
                producerTask: train-models
            metrics_path:
              taskOutputArtifact:
                outputArtifactKey: metrics_path
                producerTask: train-models
            train_data_path:
              taskOutputArtifact:
                outputArtifactKey: train_data_path
                producerTask: split-data
            val_data_path:
              taskOutputArtifact:
                outputArtifactKey: val_data_path
                producerTask: split-data
          parameters:
            n_trials:
              componentInputParameter: n_trials
            params_config_path:
              componentInputParameter: params_config_path
        taskInfo:
          name: tuning-model
      upload-model-to-vertex:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-upload-model-to-vertex
        dependentTasks:
        - evaluate-model
        - train-models
        inputs:
          artifacts:
            best_model_metrics_path:
              taskOutputArtifact:
                outputArtifactKey: evaluate_metrics_path
                producerTask: evaluate-model
            best_model_path:
              taskOutputArtifact:
                outputArtifactKey: final_tuned_model_path
                producerTask: evaluate-model
            encode_path:
              taskOutputArtifact:
                outputArtifactKey: encode_path
                producerTask: train-models
          parameters:
            model_display_name:
              componentInputParameter: model_display_name
        taskInfo:
          name: upload-model-to-vertex
  inputDefinitions:
    parameters:
      c_fn:
        defaultValue: 20000.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      c_fp:
        defaultValue: 200.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      c_review:
        defaultValue: 30.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      human_hit_rate:
        defaultValue: 0.8
        isOptional: true
        parameterType: NUMBER_DOUBLE
      model_display_name:
        defaultValue: fraud-detection-model
        isOptional: true
        parameterType: STRING
      n_trials:
        defaultValue: 50.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      params_config_path:
        parameterType: STRING
      raw_data_path:
        parameterType: STRING
      test_size:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
      train_size:
        defaultValue: 0.8
        isOptional: true
        parameterType: NUMBER_DOUBLE
      val_size:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
  outputDefinitions:
    artifacts:
      calibrate-model-scenery_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      calibrate-model-tune_model_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      evaluate-model-evaluate_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      evaluate-model-evaluate_metrics_path:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      train-models-best_model_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      train-models-metrics_path:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      tuning-model-tune_model_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.5.0
