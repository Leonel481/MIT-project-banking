{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe2b0cc-3385-4af8-ad78-7947d71a3dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from kfp import compiler, dsl\n",
    "\n",
    "# --- Variables de Entorno y Configuración ---\n",
    "PROJECT_ID = 'projectstylus01'\n",
    "REGION = 'us-central1'\n",
    "SERVICE_ACCOUNT = 'workbench-sa@projectstylus01.iam.gserviceaccount.com'\n",
    "# Su bucket de GCS donde se almacenarán los artifacts y datos procesados.\n",
    "GCS_BUCKET = 'gs://mit-project-vertex-ai-artifacts'\n",
    "PIPELINE_ROOT = f'{GCS_BUCKET}/pipeline_root/'\n",
    "PIPELINE_FILE = 'fraud_model_pipeline_experiments.yaml'\n",
    "\n",
    "# Inicializar Vertex AI\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID, \n",
    "    location=REGION, \n",
    "    service_account=SERVICE_ACCOUNT, \n",
    "    staging_bucket=GCS_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddeee5d3-3ab5-45ac-b52b-2e494b02128b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definier el nombre del experimento\n",
    "experiment = 'E1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae2ce5a7-2dcc-4f1b-9ded-6e82473b947b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos procesados guardados en: gs://mit-project-vertex-ai-artifacts/data_process/data-E1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('gs://mit-proyect-data/Base.csv')\n",
    "\n",
    "# Definiendo variables nan y categoricas\n",
    "var_nan = ['prev_address_months_count','current_address_months_count','intended_balcon_amount',\n",
    "           'bank_months_count','session_length_in_minutes','device_distinct_emails_8w']\n",
    "\n",
    "# Ingenieria de variables\n",
    "data[var_nan] = data[var_nan].replace(-1, np.nan).astype('float')\n",
    "\n",
    "data['prev_address_valid'] = np.where(data['prev_address_months_count'] > 0,1,0)\n",
    "data['velocity_6h'] = np.where(data['velocity_6h'] <= 0,data[\"velocity_6h\"].quantile(0.25),data[\"velocity_6h\"])\n",
    "data['ratio_velocity_6h_24h'] = data['velocity_6h']/data['velocity_24h']\n",
    "data['ratio_velocity_24h_4w'] = data['velocity_24h']/data['velocity_4w']\n",
    "data['log_bank_branch_count_8w'] = np.log1p(data['bank_branch_count_8w'])\n",
    "data['log_days_since_request'] = np.log1p(data['days_since_request'])\n",
    "data['prev_bank_months_count'] = np.where(data['bank_months_count'] <=0, 0, 1)\n",
    "data['income_risk_score'] = data['income']*data['credit_risk_score']\n",
    "\n",
    "data = data.drop(columns = ['device_fraud_count','month','prev_address_months_count','intended_balcon_amount', 'source'])\n",
    "\n",
    "# Guardar los datos procesados\n",
    "\n",
    "DATA_PROCESS_URI = f'{GCS_BUCKET}/data_process/data-{experiment}.csv'\n",
    "data.to_csv(DATA_PROCESS_URI, index=False)\n",
    "print(f\"Datos procesados guardados en: {DATA_PROCESS_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc7dc096-fda5-4a20-a973-8beb6f365401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForestClassifier': {'n_estimators': [100, 500],\n",
       "  'max_depth': [5, 20],\n",
       "  'max_features': ['auto', 'sqrt', 'log2'],\n",
       "  'min_samples_split': [2, 10],\n",
       "  'min_samples_leaf': [1, 5],\n",
       "  'bootstrap': [True, False],\n",
       "  'class_weight': ['None', 'balanced', 'balanced_subsample']},\n",
       " 'LGBMClassifier': {'objective': 'binary',\n",
       "  'max_depth': [3, 10],\n",
       "  'learning_rate': [0.01, 0.3],\n",
       "  'feature_fraction': [0.5, 1.0],\n",
       "  'bagging_fraction': [0.5, 1.0],\n",
       "  'num_leaves': [20, 100],\n",
       "  'lambda_l1': [0.0, 1.0],\n",
       "  'lambda_l2': [0.0, 1.0]},\n",
       " 'XGBClassifier': {'objective': 'binary:logistic',\n",
       "  'max_depth': [3, 6],\n",
       "  'learning_rate': [0.01, 0.3],\n",
       "  'n_estimators': [50, 300],\n",
       "  'subsample': [0.5, 1.0],\n",
       "  'colsample_bytree': [0.5, 1.0],\n",
       "  'reg_alpha': [0.0, 1.0],\n",
       "  'reg_lambda': [0.0, 1.0],\n",
       "  'gamma': [0.0, 1.0],\n",
       "  'eval_metric': 'aucpr'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracion de hiperparametros subido a gs://mit-project-vertex-ai-artifacts/config/experiments/config-E1.yaml listo para usar en el pipeline\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar si el cofig-template.yaml se modifico para el experimento\n",
    "import yaml\n",
    "from google.cloud import storage\n",
    "\n",
    "def config_parameters(experiment_name: str):\n",
    "    \n",
    "    template = 'config-template.yaml'\n",
    "    \n",
    "    with open(template, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    display(config)\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('mit-project-vertex-ai-artifacts')\n",
    "    destination_blob_name = f'config/experiments/config-{experiment_name}.yaml'\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(template)\n",
    "    print(f\"Configuracion de hiperparametros subido a gs://mit-project-vertex-ai-artifacts/{destination_blob_name} listo para usar en el pipeline\")\n",
    "\n",
    "\n",
    "config_parameters(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcef9316-7c26-4253-bd80-6da719aac62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Vertex AI Pipeline Job...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/435304534790/locations/us-central1/pipelineJobs/mit-project-banking-pipeline-training-20251006072120\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/435304534790/locations/us-central1/pipelineJobs/mit-project-banking-pipeline-training-20251006072120')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mit-project-banking-pipeline-training-20251006072120?project=435304534790\n",
      "PipelineJob projects/435304534790/locations/us-central1/pipelineJobs/mit-project-banking-pipeline-training-20251006072120 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "\n",
      "➡️ Revise el estado del Job aquí: projects/435304534790/locations/us-central1/pipelineJobs/mit-project-banking-pipeline-training-20251006072120\n",
      "PipelineJob projects/435304534790/locations/us-central1/pipelineJobs/mit-project-banking-pipeline-training-20251006072120 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/435304534790/locations/us-central1/pipelineJobs/mit-project-banking-pipeline-training-20251006072120 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# Se ejecuta el Job.\n",
    "import time\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Definición de Parámetros del Pipeline\n",
    "pipeline_parameters = {\n",
    "    # IMPORTANTE: Se inyecta el URI de la data procesada en el paso anterior\n",
    "    'raw_data_path': DATA_PROCESS_URI, \n",
    "    'params_config_path': f'gs://mit-project-vertex-ai-artifacts/config/experiments/config-{experiment}.yaml',\n",
    "    'model_display_name': 'fraud-model-experiment',\n",
    "    'train_size': 0.8,\n",
    "    'val_size': 0.1,\n",
    "    'test_size': 0.1,\n",
    "    'n_trials': 5, # Usar un valor más bajo para pruebas rápidas\n",
    "    'human_hit_rate': 0.85, # Tasa de acierto del revisor humano\n",
    "}\n",
    "\n",
    "# Crear y Ejecutar el Pipeline Job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name='custom-training-run',\n",
    "    template_path=PIPELINE_FILE,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=pipeline_parameters,\n",
    "    enable_caching=False, # Recomendado para pruebas con data nueva\n",
    ")\n",
    "\n",
    "print(\"Iniciando Vertex AI Pipeline Job...\")\n",
    "job.run(sync=False) # Ejecución asíncrona (no bloquea el notebook)\n",
    "\n",
    "time.sleep(10)\n",
    "print(f\"\\n➡️ Revise el estado del Job aquí: {job.resource_name}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
