# PIPELINE DEFINITION
# Name: mit-project-banking-pipeline-training
# Description: Pipeline entrenamiento de modelos para el proyecto de detecciÃ³n de fraude bancario
# Inputs:
#    model_display_name: str [Default: 'fraud-detection-model']
#    n_trials: int [Default: 50.0]
#    params_config_path: str
#    raw_data_path: str
#    test_size: float [Default: 0.1]
#    train_size: float [Default: 0.8]
#    val_size: float [Default: 0.1]
# Outputs:
#    evaluate-model-best_model_metrics: system.ClassificationMetrics
#    evaluate-model-best_model_metrics_path: system.Metrics
#    train-models-best_model_metrics: system.ClassificationMetrics
#    train-models-metrics_path: system.Metrics
#    tuning-model-tune_model_metrics: system.ClassificationMetrics
components:
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        best_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        encode_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        best_model_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        best_model_metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        final_tuned_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-load-process-data:
    executorLabel: exec-load-process-data
    inputDefinitions:
      parameters:
        raw_data_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        processed_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-split-data:
    executorLabel: exec-split-data
    inputDefinitions:
      artifacts:
        processed_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        test_size:
          defaultValue: 0.1
          isOptional: true
          parameterType: NUMBER_DOUBLE
        train_size:
          defaultValue: 0.8
          isOptional: true
          parameterType: NUMBER_DOUBLE
        val_size:
          defaultValue: 0.1
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        test_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-models:
    executorLabel: exec-train-models
    inputDefinitions:
      artifacts:
        train_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        best_model_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        encode_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-tuning-model:
    executorLabel: exec-tuning-model
    inputDefinitions:
      artifacts:
        encoder_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        train_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_data_path:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        n_trials:
          defaultValue: 50.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        params_config_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        tune_model_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        tuned_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-upload-model-to-vertex:
    executorLabel: exec-upload-model-to-vertex
    inputDefinitions:
      artifacts:
        best_model_metrics_path:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        best_model_path:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        experiment_name:
          defaultValue: fraud-detection-experiment
          isOptional: true
          parameterType: STRING
        model_display_name:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    test_data_path: Input[Dataset],\n    best_model_path:\
          \ Input[Model],\n    encode_path: Input[Model],\n    final_tuned_model_path:\
          \ Output[Model],\n    best_model_metrics: Output[ClassificationMetrics],\n\
          \    best_model_metrics_path: Output[Metrics],\n):\n\n    import pandas\
          \ as pd\n    import numpy as np\n    from sklearn.metrics import accuracy_score,\
          \ precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix,\
          \ roc_curve\n    import joblib\n    import json\n    import os\n\n    #\
          \ Cargar los datos de test\n    data_test = pd.read_csv(f'{test_data_path.path}/test_data.csv')\n\
          \n    # Cargar el encoder y modelo\n    encoder = joblib.load(f\"{encode_path.path}/encoder.joblib\"\
          )\n    best_model = joblib.load(f\"{best_model_path.path}/tuned_model.joblib\"\
          )\n\n    # Preparar los datos de test\n    cat_features = ['payment_type','employment_status','housing_status','device_os']\n\
          \    target = 'fraud_bool'\n\n    encoder_features_test = encoder.transform(data_test[cat_features])\n\
          \    encoded_df_test = pd.DataFrame(encoder_features_test, columns=encoder.get_feature_names_out(cat_features))\n\
          \n    X_test = pd.concat([data_test.drop(columns=cat_features + [target]),\
          \ encoded_df_test], axis=1)\n    y_test = data_test[target]\n\n    # Evaluar\
          \ el modelo\n    y_pred = best_model.predict(X_test)\n    # f1_score = f1_score(y_test,\
          \ y_pred)\n\n    # log the confusion matrix\n    labels = ['No Fraude',\
          \ 'Fraude']\n\n    y_pred_best = best_model.predict(X_test)\n    cm = confusion_matrix(y_test,\
          \ y_pred_best)\n    confusion_matrix_data = cm.tolist()\n\n    best_model_metrics.log_confusion_matrix(\n\
          \        categories=labels,\n        matrix=confusion_matrix_data\n    )\n\
          \n    # Model\n    os.makedirs(final_tuned_model_path.path, exist_ok=True)\n\
          \    final_tuned_model_path = final_tuned_model_path.path + \"/final_tuned_model.joblib\"\
          \n    joblib.dump(best_model, final_tuned_model_path)\n\n    # log roc auc\n\
          \    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n    fpr, tpr,\
          \ thresholds = roc_curve(y_test, y_pred_proba)\n\n    N_points = 200\n \
          \   total_points = len(fpr)\n    indices = np.linspace(0, total_points -\
          \ 1, N_points, dtype = int)\n\n    fpr = np.nan_to_num(fpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    tpr = np.nan_to_num(tpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    thresholds = np.nan_to_num(thresholds[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n\n    best_model_metrics.log_roc_curve(\n\
          \        fpr=fpr.tolist(),\n        tpr=tpr.tolist(),\n        threshold=thresholds.tolist()\n\
          \    ) \n\n    # log metric\n    model_metrics = {\n            'accuracy':\
          \ accuracy_score(y_test, y_pred),\n            'precision': precision_score(y_test,\
          \ y_pred),\n            'recall': recall_score(y_test, y_pred),\n      \
          \      'f1_score': f1_score(y_test, y_pred),\n            'roc_auc': roc_auc_score(y_test,\
          \ y_pred)\n        }\n\n    for metric, value in model_metrics.items():\n\
          \        best_model_metrics_path.log_metric(metric, value)\n\n    os.makedirs(best_model_metrics_path.path,\
          \ exist_ok=True)\n    metrics_file_path = best_model_metrics_path.path +\
          \ \"/model_metrics.json\"\n    with open(metrics_file_path, 'w') as f:\n\
          \        json.dump(model_metrics, f, indent=4)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-load-process-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_process_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_process_data(\n    raw_data_path: str,\n    processed_data_path:\
          \ Output[Dataset]\n):\n    import pandas as pd\n    import numpy as np\n\
          \    import os\n\n    # Cargar los datos\n    data = pd.read_csv(raw_data_path)\n\
          \n    # Definiendo variables nan y categoricas\n    var_nan = ['prev_address_months_count','current_address_months_count','intended_balcon_amount',\n\
          \               'bank_months_count','session_length_in_minutes','device_distinct_emails_8w']\n\
          \n    # Ingenieria de variables\n    data[var_nan] = data[var_nan].replace(-1,\
          \ np.nan).astype('float')\n\n    data['prev_address_valid'] = np.where(data['prev_address_months_count']\
          \ > 0,1,0)\n    data['velocity_6h'] = np.where(data['velocity_6h'] <= 0,data[\"\
          velocity_6h\"].quantile(0.25),data[\"velocity_6h\"])\n    data['ratio_velocity_6h_24h']\
          \ = data['velocity_6h']/data['velocity_24h']\n    data['ratio_velocity_24h_4w']\
          \ = data['velocity_24h']/data['velocity_4w']\n    data['log_bank_branch_count_8w']\
          \ = np.log1p(data['bank_branch_count_8w'])\n    data['log_days_since_request']\
          \ = np.log1p(data['days_since_request'])\n    data['prev_bank_months_count']\
          \ = np.where(data['bank_months_count'] <=0, 0, 1)\n    data['income_risk_score']\
          \ = data['income']*data['credit_risk_score']\n\n    data = data.drop(columns\
          \ = ['device_fraud_count','month','prev_address_months_count','intended_balcon_amount',\
          \ 'source'])\n\n    # Guardar los datos procesados\n    os.makedirs(processed_data_path.path,\
          \ exist_ok=True)\n    data_file_path = f\"{processed_data_path.path}/processed_data.csv\"\
          \n    data.to_csv(data_file_path, index=False)\n    print(f\"Datos procesados\
          \ guardados en: {data_file_path}\")\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-split-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - split_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef split_data(\n    processed_data_path: Input[Dataset],\n    train_data_path:\
          \ Output[Dataset],\n    test_data_path: Output[Dataset],\n    val_data_path:\
          \ Output[Dataset],\n    train_size: float = 0.8,\n    val_size: float =\
          \ 0.1,\n    test_size: float = 0.1\n):\n    import pandas as pd\n    from\
          \ sklearn.model_selection import train_test_split\n    import os\n\n   \
          \ # Cargar los datos procesados\n    data = pd.read_csv(f\"{processed_data_path.path}/processed_data.csv\"\
          )\n\n    # Diividir los datos en train, validation y test\n    train, val_test\
          \ = train_test_split(data, test_size = (1 - train_size), random_state=42,\
          \ stratify=data['fraud_bool'])\n    val, test = train_test_split(val_test,\
          \ test_size = (test_size / (val_size + test_size)), random_state=42, stratify=val_test['fraud_bool'])\n\
          \n    # Guardar los conjuntos de datos\n    os.makedirs(train_data_path.path,\
          \ exist_ok=True)\n    os.makedirs(val_data_path.path, exist_ok=True)\n \
          \   os.makedirs(test_data_path.path, exist_ok=True)\n\n    train_data_path\
          \ = train_data_path.path + \"/train_data.csv\"\n    val_data_path = val_data_path.path\
          \ + \"/val_data.csv\"\n    test_data_path = test_data_path.path + \"/test_data.csv\"\
          \n\n    train.to_csv(train_data_path, index=False)\n    val.to_csv(val_data_path,\
          \ index=False)\n    test.to_csv(test_data_path, index=False)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-train-models:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_models
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_models(\n    train_data_path: Input[Dataset],\n    val_data_path:\
          \ Input[Dataset],\n    # best_model_name: Output[str],\n    encode_path:\
          \ Output[Model],\n    best_model_metrics: Output[ClassificationMetrics],\n\
          \    # metrics_models: Output[Markdown],\n    metrics_path: Output[Metrics],\n\
          ):\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing\
          \ import OneHotEncoder\n    from sklearn.ensemble import RandomForestClassifier\n\
          \    from sklearn.metrics import accuracy_score, precision_score, recall_score,\
          \ f1_score, roc_auc_score, confusion_matrix, roc_curve\n    from xgboost\
          \ import XGBClassifier\n    from lightgbm import LGBMClassifier\n    import\
          \ joblib\n    import json\n    import os\n\n    # Cargar los datos de entrenamiento\n\
          \    data = pd.read_csv(f'{train_data_path.path}/train_data.csv')\n\n  \
          \  # Crear encoder, separar caracter\xEDsticas y etiqueta\n    cat_features\
          \ = ['payment_type','employment_status','housing_status','device_os']\n\
          \    target = 'fraud_bool'\n\n    encoder = OneHotEncoder(handle_unknown='ignore',\
          \ sparse_output=False)\n    encoder_features = encoder.fit_transform(data[cat_features])\n\
          \    encoded_df = pd.DataFrame(encoder_features, columns=encoder.get_feature_names_out(cat_features))\n\
          \n    X_train = pd.concat([data.drop(columns=cat_features + [target]), encoded_df],\
          \ axis=1)\n    y_train = data[target]\n\n    # Balancear clases\n    neg,\
          \ pos = y_train.value_counts()[0], y_train.value_counts()[1]\n    scale_pos_weight\
          \ = neg / pos\n\n    # Definir y entrenar modelos\n    models = {\n    \
          \    'RandomForestClassifier': RandomForestClassifier(\n            n_estimators=10,\n\
          \            class_weight='balanced', \n            random_state=42\n  \
          \          ),\n        # 'XGBClassifier': XGBClassifier(\n        #    \
          \ eval_metric='logloss', \n        #     scale_pos_weight=scale_pos_weight,\
          \ \n        #     random_state=42\n        #     ),\n        # 'LGBMClassifier':\
          \ LGBMClassifier(\n        #     scale_pos_weight=scale_pos_weight,\n  \
          \      #       random_state=42\n        #       )\n    }\n\n    trained_models\
          \ = {}\n    for name, model in models.items():\n        model.fit(X_train,\
          \ y_train)\n        trained_models[name] = model\n\n    # Guardar los modelos\
          \ y el encoder\n    # os.makedirs(models_path.path, exist_ok=True)\n   \
          \ os.makedirs(encode_path.path, exist_ok=True)\n    # models_path = models_path.path\
          \ + \"/trained_models.joblib\"\n    encode_path = encode_path.path + \"\
          /encoder.joblib\"\n\n    # joblib.dump(trained_models, models_path)\n  \
          \  joblib.dump(encoder, encode_path)\n\n\n    # Evaluate models\n    data_val\
          \ = pd.read_csv(f'{val_data_path.path}/val_data.csv')\n    encoder_features_val\
          \ = encoder.transform(data_val[cat_features])\n    encoded_df = pd.DataFrame(encoder_features_val,\
          \ columns=encoder.get_feature_names_out(cat_features))\n\n    X_val = pd.concat([data_val.drop(columns=cat_features\
          \ + [target]), encoded_df], axis=1)\n    y_val = data_val[target]\n\n  \
          \  all_metrics = {}\n    best_model_name = None\n    best_model = None\n\
          \    best_f1 = -1\n\n    for name, model in trained_models.items():\n  \
          \      y_pred = model.predict(X_val)\n        f1 = f1_score(y_val, y_pred)\n\
          \n        all_metrics[name] = {\n            'accuracy': accuracy_score(y_val,\
          \ y_pred),\n            'precision': precision_score(y_val, y_pred),\n \
          \           'recall': recall_score(y_val, y_pred),\n            'f1_score':\
          \ f1,\n            'roc_auc': roc_auc_score(y_val, y_pred)\n        }\n\n\
          \        if f1 > best_f1:\n            best_f1 = f1\n            best_model_name\
          \ = name\n            best_model = model\n\n    # Table in Markdown\n  \
          \  # markdown_table = \"| Modelo | Accuracy | Precision | Recall | F1 Score\
          \ | ROC AUC |\\n\"\n    # markdown_table += \"|--------|----------|-----------|--------|----------|---------|\\\
          n\"\n    # for model, metrics in all_metrics.items():\n    #     markdown_table\
          \ += f\"| {model} | {metrics['accuracy']:.4f} | {metrics['precision']:.4f}\
          \ | {metrics['recall']:.4f} | {metrics['f1_score']:.4f} | {metrics['roc_auc']:.4f}\
          \ |\\n\"\n\n    # os.makedirs(metrics_models.path, exist_ok=True)\n    #\
          \ markdown_path = metrics_models.path + \"/markdown.md\"\n    # with open(markdown_path,\
          \ \"w\") as f:\n    #     f.write(markdown_table)\n\n    # log the confusion\
          \ matrix\n    labels = ['No Fraude', 'Fraude']\n\n    y_pred_best = best_model.predict(X_val)\n\
          \    cm = confusion_matrix(y_val, y_pred_best)\n    confusion_matrix_data\
          \ = cm.tolist()\n\n    best_model_metrics.log_confusion_matrix(\n      \
          \  categories=labels,\n        matrix=confusion_matrix_data\n    )\n\n \
          \   # log roc auc\n    y_pred_proba = best_model.predict_proba(X_val)[:,\
          \ 1]\n    fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n\n    N_points\
          \ = 200\n    total_points = len(fpr)\n    indices = np.linspace(0, total_points\
          \ - 1, N_points, dtype = int)\n\n    fpr = np.nan_to_num(fpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    tpr = np.nan_to_num(tpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    thresholds = np.nan_to_num(thresholds[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n\n    best_model_metrics.log_roc_curve(\n\
          \        fpr=fpr.tolist(),\n        tpr=tpr.tolist(),\n        threshold=thresholds.tolist()\n\
          \    )\n\n    # log param metric\n\n    for name, metrics_dict in all_metrics.items():\n\
          \        metrics_path.log_metric(f'{name}_f1_score', metrics_dict.get('f1_score'))\n\
          \        metrics_path.log_metric(f'{name}_roc_auc', metrics_dict.get('roc_auc'))\n\
          \n    os.makedirs(metrics_path.path, exist_ok=True)\n    metrics_file_path\
          \ = metrics_path.path + \"/models_metrics.json\" \n    with open(metrics_file_path,\
          \ 'w') as f:\n        json.dump(all_metrics, f, indent=4)\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-tuning-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - tuning_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef tuning_model(\n    train_data_path: Input[Dataset],\n    val_data_path:\
          \ Input[Dataset],\n    encoder_path: Input[Model],\n    metrics_path: Input[Metrics],\n\
          \    params_config_path: str,\n    tuned_model_path: Output[Model],\n  \
          \  tune_model_metrics: Output[ClassificationMetrics],\n    n_trials: int\
          \ = 50,\n):\n    import os\n    import pandas as pd\n    import numpy as\
          \ np\n    import joblib\n    import optuna\n    import yaml\n    import\
          \ json\n    from sklearn.metrics import accuracy_score, precision_score,\
          \ recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n \
          \   from xgboost import XGBClassifier\n    from lightgbm import LGBMClassifier\
          \  \n    from sklearn.ensemble import RandomForestClassifier\n    import\
          \ gcsfs\n\n    # Cargar los datos de validaci\xF3n\n    data_train = pd.read_csv(f'{train_data_path.path}/train_data.csv')\n\
          \    data_val = pd.read_csv(f'{val_data_path.path}/val_data.csv')\n\n  \
          \  # Cargar el encoder\n    encoder = joblib.load(f\"{encoder_path.path}/encoder.joblib\"\
          )\n\n    # Preparar los datos de validaci\xF3n\n    cat_features = ['payment_type','employment_status','housing_status','device_os']\n\
          \    target = 'fraud_bool'\n\n    encoder_features_train = encoder.transform(data_train[cat_features])\n\
          \    encoded_df_train = pd.DataFrame(encoder_features_train, columns=encoder.get_feature_names_out(cat_features))\n\
          \n    X_train = pd.concat([data_train.drop(columns=cat_features + [target]),\
          \ encoded_df_train], axis=1)\n    y_train = data_train[target]\n\n    encoder_features_val\
          \ = encoder.transform(data_val[cat_features])\n    encoded_df_val = pd.DataFrame(encoder_features_val,\
          \ columns=encoder.get_feature_names_out(cat_features))\n\n    X_val = pd.concat([data_val.drop(columns=cat_features\
          \ + [target]), encoded_df_val], axis=1)\n    y_val = data_val[target]\n\n\
          \    # Balancear clases\n    neg, pos = y_train.value_counts()[0], y_train.value_counts()[1]\n\
          \    scale_pos_weight = neg / pos\n\n    # Cargar el yaml de hiperpar\xE1\
          metros\n    def download_yaml_from_gcs(gcs_path: str, local_path: str =\
          \ \"config.yaml\"):\n        fs = gcsfs.GCSFileSystem()\n        fs.get(gcs_path,\
          \ local_path)\n\n        with open(local_path, \"r\") as f:\n          \
          \  config = yaml.safe_load(f)\n        return config\n\n    params_config\
          \ = download_yaml_from_gcs(params_config_path)\n\n    # Load metrics json\n\
          \    metrics_file = os.path.join(metrics_path.path, os.listdir(metrics_path.path)[0])\n\
          \    with open(metrics_file, \"r\") as f:\n        metrics = json.load(f)\n\
          \n    best_model = max(metrics.items(), key=lambda x: x[1][\"f1_score\"\
          ])\n    best_model_name = best_model[0]\n\n    # Definir hiperpar\xE1metros\
          \ para ajustar\n    def objective(trial, params_config=params_config):\n\
          \n        param_type = {\n            'n_estimators': 'int', 'max_depth':\
          \ 'int', 'min_samples_split': 'int',\n            'min_samples_leaf': 'int',\
          \ 'num_leaves': 'int',\n            'learning_rate': 'float', 'subsample':\
          \ 'float', 'colsample_bytree': 'float',\n            'reg_alpha': 'float',\
          \ 'reg_lambda': 'float', 'gamma': 'float',\n            'feature_fraction':\
          \ 'float', 'bagging_fraction': 'float', 'lambda_l1': 'float',\n        \
          \    'lambda_l2': 'float',\n            'max_features': 'categorical', 'bootstrap':\
          \ 'categorical', 'class_weight': 'categorical',\n            'objective':\
          \ 'constant'\n        }\n\n        params = {}\n\n        model_config =\
          \ params_config.get(best_model_name)\n\n        for param_name, value in\
          \ model_config.items():\n\n            if param_type.get(param_name) ==\
          \ 'int':\n                params[param_name] = trial.suggest_int(param_name,\
          \ value[0], value[1])\n            elif param_type.get(param_name) == 'float':\n\
          \                params[param_name] = trial.suggest_float(param_name, value[0],\
          \ value[1])\n            elif param_type.get(param_name) == 'categorical':\n\
          \                params[param_name] = trial.suggest_categorical(param_name,\
          \ value)\n            elif param_type.get(param_name) == 'constant':\n \
          \               params[param_name] = value\n            else:\n        \
          \        raise ValueError(f\"Tipo de par\xE1metro no soportado: {param_type[param_name]}\"\
          )\n\n        if best_model_name in ['LGBMClassifier', 'XGBClassifier']:\n\
          \            params['scale_pos_weight'] = scale_pos_weight\n\n        #\
          \ Crear la instancia del modelo din\xE1micamente\n        if best_model_name\
          \ == 'RandomForestClassifier':\n            model = RandomForestClassifier(**params)\n\
          \        elif best_model_name == 'LGBMClassifier':\n            model =\
          \ LGBMClassifier(**params)\n        elif best_model_name == 'XGBClassifier':\n\
          \            model = XGBClassifier(**params)\n        else:\n          \
          \  raise ValueError(f\"Modelo no soportado para tuning: {best_model_name}\"\
          )\n\n        # Condicional para fit()\n\n        if best_model_name in ['LGBMClassifier',\
          \ 'XGBClassifier']:\n            model.fit(X_train, y_train,\n         \
          \             eval_set=[(X_val, y_val)],\n                      verbose=False,\n\
          \                      early_stopping_rounds=10)\n\n        else:\n    \
          \        model.fit(X_train, y_train)\n\n        y_pred = model.predict(X_val)\n\
          \        f1 = f1_score(y_val, y_pred)\n\n        return f1\n\n    # Ejecutar\
          \ la optimizaci\xF3n de hiperpar\xE1metros\n    study = optuna.create_study(direction='maximize')\n\
          \    study.optimize(objective, n_trials=n_trials)\n\n    # Entrenar el modelo\
          \ con los mejores hiperpar\xE1metros\n    best_params = study.best_trial.params\n\
          \    tuned_model = XGBClassifier(**best_params, random_state=42)\n    tuned_model.fit(X_val,\
          \ y_val)\n\n    # Guardar el modelo ajustado\n    os.makedirs(tuned_model_path.path,\
          \ exist_ok=True)\n    tuned_model_file = tuned_model_path.path + \"/tuned_model.joblib\"\
          \n    joblib.dump(tuned_model, tuned_model_file)\n\n    # Metricas del modelo\
          \ ajustado\n    # log the confusion matrix\n    labels = ['No Fraude', 'Fraude']\n\
          \n    y_pred_best = tuned_model.predict(X_val)\n    cm = confusion_matrix(y_val,\
          \ y_pred_best)\n    confusion_matrix_data = cm.tolist()\n\n    tune_model_metrics.log_confusion_matrix(\n\
          \        categories=labels,\n        matrix=confusion_matrix_data\n    )\n\
          \n    # log roc auc\n    y_pred_proba = tuned_model.predict_proba(X_val)[:,\
          \ 1]\n    fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n\n    N_points\
          \ = 200\n    total_points = len(fpr)\n    indices = np.linspace(0, total_points\
          \ - 1, N_points, dtype = int)\n\n    fpr = np.nan_to_num(fpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    tpr = np.nan_to_num(tpr[indices], nan=0.0,\
          \ posinf=1.0, neginf=0.0)\n    thresholds = np.nan_to_num(thresholds[indices],\
          \ nan=0.0, posinf=1.0, neginf=0.0)\n\n    tune_model_metrics.log_roc_curve(\n\
          \        fpr=fpr.tolist(),\n        tpr=tpr.tolist(),\n        threshold=thresholds.tolist()\n\
          \    )\n\n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
    exec-upload-model-to-vertex:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_vertex
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.5.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_vertex(\n    best_model_path: Input[Model],\n\
          \    best_model_metrics_path: Input[Metrics],\n    model_display_name: str,\n\
          \    experiment_name: str = 'fraud-detection-experiment'\n):\n    import\
          \ google.cloud.aiplatform as aiplatform\n    import datetime\n    import\
          \ json\n    import os\n\n    # Inicializar Vertex AI\n    aiplatform.init()\n\
          \n    metrics_file = os.path.join(best_model_metrics_path.path, os.listdir(best_model_metrics_path.path)[0])\n\
          \    with open(metrics_file, \"r\") as f:\n        metrics = json.load(f)\n\
          \n    # Crear Experimento\n    aiplatform.init(experiment=experiment_name)\n\
          \    run_name = f\"run-{model_display_name}-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\
          \n    run = aiplatform.start_run(run = run_name)\n\n    # Subir el modelo\
          \ a Vertex AI\n    artifact = aiplatform.Model.upload(\n        display_name=model_display_name,\n\
          \        artifact_uri=best_model_path.path.rsplit('/', 1)[0],\n        serving_container_image_uri='us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest'\n\
          \    )\n\n    aiplatform.log_metrics(metrics)\n\n    aiplatform.end_run()\n\
          \n    print(f'Modelo subido a Vertex AI con ID: {artifact.resource_name}')\n\
          \n"
        image: us-central1-docker.pkg.dev/projectstylus01/vertex/mit-project-custom:latest
pipelineInfo:
  description: "Pipeline entrenamiento de modelos para el proyecto de detecci\xF3\
    n de fraude bancario"
  name: mit-project-banking-pipeline-training
root:
  dag:
    outputs:
      artifacts:
        evaluate-model-best_model_metrics:
          artifactSelectors:
          - outputArtifactKey: best_model_metrics
            producerSubtask: evaluate-model
        evaluate-model-best_model_metrics_path:
          artifactSelectors:
          - outputArtifactKey: best_model_metrics_path
            producerSubtask: evaluate-model
        train-models-best_model_metrics:
          artifactSelectors:
          - outputArtifactKey: best_model_metrics
            producerSubtask: train-models
        train-models-metrics_path:
          artifactSelectors:
          - outputArtifactKey: metrics_path
            producerSubtask: train-models
        tuning-model-tune_model_metrics:
          artifactSelectors:
          - outputArtifactKey: tune_model_metrics
            producerSubtask: tuning-model
    tasks:
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - split-data
        - train-models
        - tuning-model
        inputs:
          artifacts:
            best_model_path:
              taskOutputArtifact:
                outputArtifactKey: tuned_model_path
                producerTask: tuning-model
            encode_path:
              taskOutputArtifact:
                outputArtifactKey: encode_path
                producerTask: train-models
            test_data_path:
              taskOutputArtifact:
                outputArtifactKey: test_data_path
                producerTask: split-data
        taskInfo:
          name: evaluate-model
      load-process-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-process-data
        inputs:
          parameters:
            raw_data_path:
              componentInputParameter: raw_data_path
        taskInfo:
          name: load-process-data
      split-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-split-data
        dependentTasks:
        - load-process-data
        inputs:
          artifacts:
            processed_data_path:
              taskOutputArtifact:
                outputArtifactKey: processed_data_path
                producerTask: load-process-data
          parameters:
            test_size:
              componentInputParameter: test_size
            train_size:
              componentInputParameter: train_size
            val_size:
              componentInputParameter: val_size
        taskInfo:
          name: split-data
      train-models:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-models
        dependentTasks:
        - split-data
        inputs:
          artifacts:
            train_data_path:
              taskOutputArtifact:
                outputArtifactKey: train_data_path
                producerTask: split-data
            val_data_path:
              taskOutputArtifact:
                outputArtifactKey: val_data_path
                producerTask: split-data
        taskInfo:
          name: train-models
      tuning-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-tuning-model
        dependentTasks:
        - split-data
        - train-models
        inputs:
          artifacts:
            encoder_path:
              taskOutputArtifact:
                outputArtifactKey: encode_path
                producerTask: train-models
            metrics_path:
              taskOutputArtifact:
                outputArtifactKey: metrics_path
                producerTask: train-models
            train_data_path:
              taskOutputArtifact:
                outputArtifactKey: train_data_path
                producerTask: split-data
            val_data_path:
              taskOutputArtifact:
                outputArtifactKey: val_data_path
                producerTask: split-data
          parameters:
            n_trials:
              componentInputParameter: n_trials
            params_config_path:
              componentInputParameter: params_config_path
        taskInfo:
          name: tuning-model
      upload-model-to-vertex:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-upload-model-to-vertex
        dependentTasks:
        - evaluate-model
        inputs:
          artifacts:
            best_model_metrics_path:
              taskOutputArtifact:
                outputArtifactKey: best_model_metrics_path
                producerTask: evaluate-model
            best_model_path:
              taskOutputArtifact:
                outputArtifactKey: final_tuned_model_path
                producerTask: evaluate-model
          parameters:
            model_display_name:
              componentInputParameter: model_display_name
        taskInfo:
          name: upload-model-to-vertex
  inputDefinitions:
    parameters:
      model_display_name:
        defaultValue: fraud-detection-model
        isOptional: true
        parameterType: STRING
      n_trials:
        defaultValue: 50.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      params_config_path:
        parameterType: STRING
      raw_data_path:
        parameterType: STRING
      test_size:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
      train_size:
        defaultValue: 0.8
        isOptional: true
        parameterType: NUMBER_DOUBLE
      val_size:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
  outputDefinitions:
    artifacts:
      evaluate-model-best_model_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      evaluate-model-best_model_metrics_path:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      train-models-best_model_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      train-models-metrics_path:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      tuning-model-tune_model_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.5.0
